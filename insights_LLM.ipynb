{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\enwin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\llms\\openai.py:249: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "c:\\Users\\enwin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\langchain_community\\llms\\openai.py:1070: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing context\n",
      "insights and summary: Detailed insights from the transcript include:\n",
      "1. The interviewee finds the app user-friendly but believes it could be enhanced with more personalized features.\n",
      "2. The interviewee specifically mentions wanting customizable notifications and tailored content based on preferences for a more fulfilling experience.\n",
      "3. The interviewee notes that the app occasionally lags during peak hours and suggests improving its speed.\n",
      "4. The interviewee also mentions that the search function could be more accurate, as it sometimes misses relevant results.\n",
      "5. Overall, the interviewee is satisfied with the app but believes that a few tweaks would make it even better.\n",
      "\n",
      "Key focus points for improvement based on the feedback provided:\n",
      "1. Personalization: Implement customizable notifications and tailored content based on user preferences.\n",
      "2. Performance: Optimize the app's speed to prevent lags during peak hours.\n",
      "3. Search function: Refine the search algorithm to ensure more accurate and relevant results.\n",
      "4. User experience: Continuously seek feedback from users to identify areas for improvement and make necessary tweaks to enhance the overall app experience.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI  \n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import fitz \n",
    "\n",
    "# Replace with your OpenAI API key\n",
    "openai_api_key = \"openAI API KEY\"\n",
    "model_name = \"gpt-3.5-turbo-0125\"  # Choose appropriate model (consider limitatins)\n",
    "\n",
    "llm = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "def process_uploaded_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    pdf_text=\"\"\n",
    "    text_array=[]\n",
    "    with fitz.open(pdf_path) as pdf_document:\n",
    "        for page_number in range(len(pdf_document)):\n",
    "            page = pdf_document.load_page(page_number)\n",
    "            text = page.get_text() # Add newline after each page's text\n",
    "            for i in text:\n",
    "                if i != \"\\n\":\n",
    "                  text_array.append(i)\n",
    "                else:\n",
    "                    text_array.append(\" \")\n",
    "            text =''.join(text_array)\n",
    "            text=text.replace(\" Interviewer:\", \"\\nInterviewer:\").replace(\" Interviewee:\", \"\\nInterviewee:\")\n",
    "            # text=text.strip(\"Transcript:- \")\n",
    "            # print(text)\n",
    "            pdf_text += text  # Add newline after each page's text\n",
    "    return pdf_text\n",
    "\n",
    "\n",
    "def generate_response(llm_model, prompt):\n",
    "    response = llm_model.generate(\n",
    "        prompts=prompt,\n",
    "        max_tokens=1024,  # Adjust maximum response length as needed\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7  # Controls response randomness (0 - deterministic, 1 - random)\n",
    "    )\n",
    "    return response #.text.strip()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    context=process_uploaded_pdf(\"D:/assignment_v1.pdf\")\n",
    "    print(\"printing context\")\n",
    "    user_question = \"Based on the transcript what detailed insights can we draw from this and what are the key focus points?\"\n",
    "    prompt = f\"Here is some relevant information from the PDF: {context}. {user_question}\"\n",
    "    prompt = [prompt]\n",
    "    response = generate_response(llm, prompt)\n",
    "    response = response.generations[0]\n",
    "    response = response[0].text \n",
    "    print(f\"insights and summary: {response}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/Enwin-A/insights-LLM/blob/main/insights_LLM.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
